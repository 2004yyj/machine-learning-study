{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"history_visible":true,"authorship_tag":"ABX9TyMLrBqXoJIPDZv2r4Cb5Z86"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.data as torchdata\n","\n","from torchvision import transforms, datasets"],"metadata":{"id":"F7I65hEs6nzW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if torch.cuda.is_available():\n","  DEVICE = torch.device('cuda')\n","else:\n","  DEVICE = torch.device('cpu')\n","\n","print(f'Using Pytorch version: {torch.__version__}, Device: {DEVICE}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jeG_ev1Q1S7q","executionInfo":{"status":"ok","timestamp":1682307744351,"user_tz":-540,"elapsed":70,"user":{"displayName":"YoonJae Yang","userId":"02319418088715156839"}},"outputId":"b37a2cc2-fbb7-4727-f3b2-432e8f2042fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using Pytorch version: 2.0.0+cu118, Device: cuda\n"]}]},{"cell_type":"code","source":["# BATCH_SIZE -> 1개의 Mini-Batch 단위에 대해서 구성된 데이터의 갯수\n","# EPOCHS -> 존재하고 있는 Mini-Batch를 전부 이용해서 학습한 횟수\n","\n","# Iteration -> 1개의 Mini-Batch를 통해 학습한 횟수\n","\n","# 예를 들어, 전체 데이터가 1만개, Batch Size가 1000개일 때,\n","# 학습은 1 Epoch당 Iteration이 10회 발생함\n","\n","BATCH_SIZE = 32\n","EPOCHS = 10"],"metadata":{"id":"m4O8RSMn1uX7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = datasets.CIFAR10(\n","    root = \"../data/CIFAR_10\", # 저장할 디렉토리 지정\n","    train = True, # Train 데이터셋인지 Test 데이터셋인지 지정\n","    download = True, # 해당 데이터를 인터넷에서 다운로드해 이용할 것인지 지정\n","    transform = transforms.Compose([\n","        transforms.RandomHorizontalFlip(), # 이미지를 50% 확률로 좌우반전\n","        transforms.ToTensor(),  # 0~255값으로 이뤄진 픽셀값을 0~1로 변환하는 작업\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Tensor로 변화한 이미지에 정규화를 진행\n","    ])\n",")\n","\n","test_dataset = datasets.CIFAR10(\n","    root = \"../data/CIFAR_10\",\n","    train = False,\n","    transform = transforms.Compose([\n","        transforms.RandomHorizontalFlip(), # 이미지를 50% 확률로 좌우반전\n","        transforms.ToTensor(),  # 0~255값으로 이뤄진 픽셀값을 0~1로 변환하는 작업\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Tensor로 변화한 이미지에 정규화를 진행\n","    ])\n",")\n","\n","# 데이터를 iterator 객체로 변환시켜주는 함수\n","train_loader = torchdata.DataLoader(\n","    dataset = train_dataset,\n","    batch_size = BATCH_SIZE,\n","    shuffle = True\n",")\n","\n","test_loader = torchdata.DataLoader(\n","    dataset = train_dataset,\n","    batch_size = BATCH_SIZE,\n","    shuffle = False\n",")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N9DRQj_G1udF","executionInfo":{"status":"ok","timestamp":1682307745714,"user_tz":-540,"elapsed":1430,"user":{"displayName":"YoonJae Yang","userId":"02319418088715156839"}},"outputId":"81c9da95-1ae1-4679-a58d-7724e9d0a3d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n"]}]},{"cell_type":"code","source":["class Net(nn.Module):\n","  def __init__(self):\n","    super(Net, self).__init__()\n","    self.fc1 = nn.Linear(8 * 8 * 16, 64)\n","    self.fc2 = nn.Linear(64, 32)\n","    self.fc3 = nn.Linear(32, 10)\n","\n","  def forward(self, x):\n","    # 2차원 배열을 Flatten\n","    x = x.view(-1, 8 * 8 * 16)\n","    x = self.fc1(x)\n","    x = F.relu(x)\n","    x = self.fc2(x)\n","    x = F.relu(x)\n","    x = self.fc3(x)\n","    x = F.relu(x)\n","    # 분류 확률 계산함수\n","    x = F.log_softmax(x, dim = 1)\n","    return x"],"metadata":{"id":"wSuj3SNM84Gv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Net().to(DEVICE)\n","optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5) # output, lr(learning rate): 학습률\n","criterion = nn.CrossEntropyLoss() # 모델의 예측값과 분류된 실제값의 차이 계산\n","\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HGV4ogH5-sP3","executionInfo":{"status":"ok","timestamp":1682307797617,"user_tz":-540,"elapsed":22,"user":{"displayName":"YoonJae Yang","userId":"02319418088715156839"}},"outputId":"ba6af3a4-c673-48bb-b823-203c72ed17c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Net(\n","  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (fc1): Linear(in_features=1024, out_features=64, bias=True)\n","  (fc2): Linear(in_features=64, out_features=32, bias=True)\n","  (fc3): Linear(in_features=32, out_features=10, bias=True)\n",")\n"]}]},{"cell_type":"code","source":["def train(model, train_loader, optimizer, log_interval):\n","  model.train() # 정의한 mlp 모델을 학습상태로 지정\n","  for batch_idx, (image, label) in enumerate(train_loader): # enumerate(리스트 인덱스 반환)\n","    # 이미지/레이블을 장비에 할당\n","    image = image.to(DEVICE)\n","    label = label.to(DEVICE)\n","    # optimizer 비우기\n","    optimizer.zero_grad()\n","    # 장비에 할당한 이미지 데이터를 MLP Input에 지정\n","    output = model(image)\n","    # 계산된 예측값과 레이블 데이터로 Loss 값 계산\n","    loss = criterion(output, label)\n","    loss.backward() # Back Propagation으로 계산된 Gradient 값 할당\n","    optimizer.step()\n","\n","    if batch_idx % log_interval == 0:\n","      print('Train Epoch: {} [{}/{}({:.0f}%)]\\tTrain Loss: {:.6f}'.format(\n","          Epoch, \n","          batch_idx * len(image), \n","          len(train_loader.dataset), \n","          100. * batch_idx / len(train_loader),\n","          loss.item()))\n","    "],"metadata":{"id":"ZXpwrSdP_GGa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, test_loader):\n","  model.eval() # 정의한 mlp 모델을 평가상태로 지정\n","  test_loss = 0 # test_loader 내의 데이터로 Loss 값을 계산하기 위해서 test_loss를 0으로 지정\n","  correct = 0 # 올바른 class로 분류된 경우를 계산하기 위해 correct를 0으로 임시 지정\n","\n","  with torch.no_grad(): # 평가상태에서 계산된 Gradient 값이 저장되지 않도록 설정\n","    for image, label in test_loader:\n","       # 이미지/레이블을 장비에 할당\n","      image = image.to(DEVICE)\n","      label = label.to(DEVICE)\n","      # 장비에 할당한 이미지 데이터를 MLP Input에 지정\n","      output = model(image)\n","      # 계산된 예측값과 레이블 데이터로 Loss 값 추가\n","      test_loss += criterion(output, label).item()\n","      # output 벡터 내에서 가장 큰 값이 같다면 예측 성공으로 판단\n","      prediction = output.max(1, keepdim = True)[1]\n","      # 예측 성공 시 correct값을 더하기\n","      correct += prediction.eq(label.view_as(prediction)).sum().item()\n","\n","  # test_loss 값을 test_loader 내에 있는 Mini-Batch 개수만큼 나눠 평균 Loss값으로 계산\n","  test_loss /= len(test_loader.dataset)\n","  test_accuracy = 100. * correct / len(test_loader.dataset) # test_loader 데이터 중 얼마나 맞춘 지 정확도 계산\n","  return test_loss, test_accuracy"],"metadata":{"id":"WFkR40t-5NHa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for Epoch in range(1, EPOCHS + 1):\n","  train(model, train_loader, optimizer, log_interval = 200)\n","  test_loss, test_accuracy = evaluate(model, test_loader)\n","  print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(Epoch, test_loss, test_accuracy))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EETXPIzc9mmX","executionInfo":{"status":"ok","timestamp":1682308195812,"user_tz":-540,"elapsed":392252,"user":{"displayName":"YoonJae Yang","userId":"02319418088715156839"}},"outputId":"9d830a1d-e6b4-48dd-d94e-6b875ad6bb2a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [0/50000(0%)]\tTrain Loss: 2.306349\n","Train Epoch: 1 [6400/50000(13%)]\tTrain Loss: 2.295599\n","Train Epoch: 1 [12800/50000(26%)]\tTrain Loss: 2.287020\n","Train Epoch: 1 [19200/50000(38%)]\tTrain Loss: 2.277914\n","Train Epoch: 1 [25600/50000(51%)]\tTrain Loss: 2.304658\n","Train Epoch: 1 [32000/50000(64%)]\tTrain Loss: 2.223799\n","Train Epoch: 1 [38400/50000(77%)]\tTrain Loss: 2.108488\n","Train Epoch: 1 [44800/50000(90%)]\tTrain Loss: 2.117427\n","\n","[EPOCH: 1], \tTest Loss: 0.0640, \tTest Accuracy: 28.26 % \n","\n","Train Epoch: 2 [0/50000(0%)]\tTrain Loss: 2.102288\n","Train Epoch: 2 [6400/50000(13%)]\tTrain Loss: 2.067586\n","Train Epoch: 2 [12800/50000(26%)]\tTrain Loss: 1.913772\n","Train Epoch: 2 [19200/50000(38%)]\tTrain Loss: 1.978625\n","Train Epoch: 2 [25600/50000(51%)]\tTrain Loss: 1.744250\n","Train Epoch: 2 [32000/50000(64%)]\tTrain Loss: 1.728637\n","Train Epoch: 2 [38400/50000(77%)]\tTrain Loss: 1.681114\n","Train Epoch: 2 [44800/50000(90%)]\tTrain Loss: 1.414682\n","\n","[EPOCH: 2], \tTest Loss: 0.0509, \tTest Accuracy: 41.22 % \n","\n","Train Epoch: 3 [0/50000(0%)]\tTrain Loss: 1.617514\n","Train Epoch: 3 [6400/50000(13%)]\tTrain Loss: 1.564174\n","Train Epoch: 3 [12800/50000(26%)]\tTrain Loss: 1.383517\n","Train Epoch: 3 [19200/50000(38%)]\tTrain Loss: 1.352265\n","Train Epoch: 3 [25600/50000(51%)]\tTrain Loss: 1.145342\n","Train Epoch: 3 [32000/50000(64%)]\tTrain Loss: 1.767184\n","Train Epoch: 3 [38400/50000(77%)]\tTrain Loss: 1.559013\n","Train Epoch: 3 [44800/50000(90%)]\tTrain Loss: 1.418055\n","\n","[EPOCH: 3], \tTest Loss: 0.0423, \tTest Accuracy: 51.51 % \n","\n","Train Epoch: 4 [0/50000(0%)]\tTrain Loss: 1.260575\n","Train Epoch: 4 [6400/50000(13%)]\tTrain Loss: 1.469949\n","Train Epoch: 4 [12800/50000(26%)]\tTrain Loss: 1.358875\n","Train Epoch: 4 [19200/50000(38%)]\tTrain Loss: 1.383206\n","Train Epoch: 4 [25600/50000(51%)]\tTrain Loss: 1.174616\n","Train Epoch: 4 [32000/50000(64%)]\tTrain Loss: 1.273362\n","Train Epoch: 4 [38400/50000(77%)]\tTrain Loss: 1.779271\n","Train Epoch: 4 [44800/50000(90%)]\tTrain Loss: 1.483738\n","\n","[EPOCH: 4], \tTest Loss: 0.0404, \tTest Accuracy: 54.33 % \n","\n","Train Epoch: 5 [0/50000(0%)]\tTrain Loss: 1.305723\n","Train Epoch: 5 [6400/50000(13%)]\tTrain Loss: 0.952453\n","Train Epoch: 5 [12800/50000(26%)]\tTrain Loss: 1.211717\n","Train Epoch: 5 [19200/50000(38%)]\tTrain Loss: 1.223553\n","Train Epoch: 5 [25600/50000(51%)]\tTrain Loss: 1.128744\n","Train Epoch: 5 [32000/50000(64%)]\tTrain Loss: 1.098860\n","Train Epoch: 5 [38400/50000(77%)]\tTrain Loss: 1.226106\n","Train Epoch: 5 [44800/50000(90%)]\tTrain Loss: 1.190066\n","\n","[EPOCH: 5], \tTest Loss: 0.0368, \tTest Accuracy: 58.33 % \n","\n","Train Epoch: 6 [0/50000(0%)]\tTrain Loss: 1.057962\n","Train Epoch: 6 [6400/50000(13%)]\tTrain Loss: 0.937728\n","Train Epoch: 6 [12800/50000(26%)]\tTrain Loss: 0.923804\n","Train Epoch: 6 [19200/50000(38%)]\tTrain Loss: 1.259346\n","Train Epoch: 6 [25600/50000(51%)]\tTrain Loss: 1.338853\n","Train Epoch: 6 [32000/50000(64%)]\tTrain Loss: 0.855883\n","Train Epoch: 6 [38400/50000(77%)]\tTrain Loss: 0.672485\n","Train Epoch: 6 [44800/50000(90%)]\tTrain Loss: 1.120853\n","\n","[EPOCH: 6], \tTest Loss: 0.0364, \tTest Accuracy: 58.50 % \n","\n","Train Epoch: 7 [0/50000(0%)]\tTrain Loss: 1.205656\n","Train Epoch: 7 [6400/50000(13%)]\tTrain Loss: 1.159563\n","Train Epoch: 7 [12800/50000(26%)]\tTrain Loss: 0.746171\n","Train Epoch: 7 [19200/50000(38%)]\tTrain Loss: 0.967704\n","Train Epoch: 7 [25600/50000(51%)]\tTrain Loss: 1.012801\n","Train Epoch: 7 [32000/50000(64%)]\tTrain Loss: 0.966255\n","Train Epoch: 7 [38400/50000(77%)]\tTrain Loss: 1.316424\n","Train Epoch: 7 [44800/50000(90%)]\tTrain Loss: 0.970565\n","\n","[EPOCH: 7], \tTest Loss: 0.0332, \tTest Accuracy: 62.24 % \n","\n","Train Epoch: 8 [0/50000(0%)]\tTrain Loss: 0.839431\n","Train Epoch: 8 [6400/50000(13%)]\tTrain Loss: 1.139829\n","Train Epoch: 8 [12800/50000(26%)]\tTrain Loss: 1.021685\n","Train Epoch: 8 [19200/50000(38%)]\tTrain Loss: 0.819347\n","Train Epoch: 8 [25600/50000(51%)]\tTrain Loss: 1.112440\n","Train Epoch: 8 [32000/50000(64%)]\tTrain Loss: 1.005327\n","Train Epoch: 8 [38400/50000(77%)]\tTrain Loss: 1.281931\n","Train Epoch: 8 [44800/50000(90%)]\tTrain Loss: 0.839711\n","\n","[EPOCH: 8], \tTest Loss: 0.0307, \tTest Accuracy: 65.08 % \n","\n","Train Epoch: 9 [0/50000(0%)]\tTrain Loss: 1.253587\n","Train Epoch: 9 [6400/50000(13%)]\tTrain Loss: 0.906047\n","Train Epoch: 9 [12800/50000(26%)]\tTrain Loss: 1.042862\n","Train Epoch: 9 [19200/50000(38%)]\tTrain Loss: 1.427599\n","Train Epoch: 9 [25600/50000(51%)]\tTrain Loss: 1.141242\n","Train Epoch: 9 [32000/50000(64%)]\tTrain Loss: 1.308399\n","Train Epoch: 9 [38400/50000(77%)]\tTrain Loss: 0.828349\n","Train Epoch: 9 [44800/50000(90%)]\tTrain Loss: 1.040836\n","\n","[EPOCH: 9], \tTest Loss: 0.0301, \tTest Accuracy: 65.74 % \n","\n","Train Epoch: 10 [0/50000(0%)]\tTrain Loss: 1.177629\n","Train Epoch: 10 [6400/50000(13%)]\tTrain Loss: 1.091233\n","Train Epoch: 10 [12800/50000(26%)]\tTrain Loss: 0.955444\n","Train Epoch: 10 [19200/50000(38%)]\tTrain Loss: 0.815852\n","Train Epoch: 10 [25600/50000(51%)]\tTrain Loss: 0.856767\n","Train Epoch: 10 [32000/50000(64%)]\tTrain Loss: 0.971796\n","Train Epoch: 10 [38400/50000(77%)]\tTrain Loss: 1.074767\n","Train Epoch: 10 [44800/50000(90%)]\tTrain Loss: 0.926916\n","\n","[EPOCH: 10], \tTest Loss: 0.0301, \tTest Accuracy: 65.94 % \n","\n"]}]}]}